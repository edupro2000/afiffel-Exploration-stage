{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consolidated-underwear",
   "metadata": {},
   "source": [
    "# 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assigned-background",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['[Verse 1]', 'They come from everywhere', 'A longing to be free', 'They come to join us here', 'From sea to shining sea And they all have a dream', 'As people always will', 'To be safe and warm', 'In that shining city on the hill Some wanna slam the door', 'Instead of opening the gate', \"Aw, let's turn this thing around\"]\n"
     ]
    }
   ],
   "source": [
    "#라이브러리 파일 준비\n",
    "import glob\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:        \n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-instrumentation",
   "metadata": {},
   "source": [
    "1. 데이터 정제\n",
    "\n",
    "\n",
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
    "\n",
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
    "\n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수도 있겠죠.\n",
    "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 를 권합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "documented-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "#필터링\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "horizontal-blues",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> verse <end>',\n",
       " '<start> they come from everywhere <end>',\n",
       " '<start> a longing to be free <end>',\n",
       " '<start> they come to join us here <end>',\n",
       " '<start> from sea to shining sea and they all have a dream <end>',\n",
       " '<start> as people always will <end>',\n",
       " '<start> to be safe and warm <end>',\n",
       " '<start> in that shining city on the hill some wanna slam the door <end>',\n",
       " '<start> instead of opening the gate <end>',\n",
       " '<start> aw , let s turn this thing around <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  정제 데이터 구축--준비 끝\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "generic-parish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  520    3 ...    0    0    0]\n",
      " [   2   45   66 ...    0    0    0]\n",
      " [   2    9 3390 ...    0    0    0]\n",
      " ...\n",
      " [   2  561   21 ...    0    0    0]\n",
      " [   2  120   34 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f1f7a174ed0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, #단어장 크기 12000이상\n",
    "        filters=' ',  \n",
    "        oov_token=\"<unk>\"  \n",
    "  \n",
    "    )\n",
    "   \n",
    "    tokenizer.fit_on_texts(corpus)  \n",
    "    tensor_tmp = tokenizer.texts_to_sequences(corpus) \n",
    "    \n",
    "    tensor=[]\n",
    "    for i in tensor_tmp:\n",
    "        if len(i) > 15:\n",
    "            continue\n",
    "        else:\n",
    "            tensor.append(i)\n",
    "            \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attached-mistake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verse 1]\n",
      "They come from everywhere\n",
      "A longing to be free\n",
      "They come to join us here\n",
      "From sea to shining sea And they all have a dream\n",
      "As people always will\n",
      "To be safe and warm\n",
      "In that shining city on the hill Some wanna slam the door\n",
      "Instead of opening the gate\n",
      "Aw, let's turn this thing around\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial-august",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liable-aircraft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 520   3   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[520   3   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "#단어사전 구축\n",
    "src_input = tensor[:, :-1]   #tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성, 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높음\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "opposite-weekend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#텐서 객체 생성--데이터 전처리 끝!!\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))  \n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-society",
   "metadata": {},
   "source": [
    "2. 평가 데이터셋 분리\n",
    "훈련 데이터와 평가 데이터를 분리하세요!\n",
    "\n",
    "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상 으로 설정하세요! 총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subjective-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]  \n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=20)\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-orchestra",
   "metadata": {},
   "source": [
    "3. 인공지능 만들기\n",
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)\n",
    "\n",
    "그리고 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "educated-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 1256\n",
    "hidden_size = 2000\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "latest-potato",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-2.6503956e-04, -8.0106338e-04, -2.2795475e-04, ...,\n",
       "         -4.0653371e-04,  2.6256189e-04, -2.0669219e-04],\n",
       "        [-1.2845028e-04, -1.1860488e-03, -3.6882932e-04, ...,\n",
       "         -1.0265789e-03,  1.1403528e-03,  8.7334984e-06],\n",
       "        [-3.3646566e-04, -1.0830673e-03, -4.1869804e-04, ...,\n",
       "         -1.5648289e-03,  1.7942192e-03, -6.0915254e-04],\n",
       "        ...,\n",
       "        [-3.4838081e-03,  2.3006569e-03,  2.6971448e-04, ...,\n",
       "         -2.6644810e-04,  9.0722681e-04, -1.8178547e-03],\n",
       "        [-4.1034501e-03,  2.9375067e-03,  6.0810975e-04, ...,\n",
       "          9.6332617e-05,  5.7243352e-04, -1.9354866e-03],\n",
       "        [-4.6590259e-03,  3.4941996e-03,  8.9064625e-04, ...,\n",
       "          3.7504040e-04,  2.5579936e-04, -2.0063724e-03]],\n",
       "\n",
       "       [[-2.6503956e-04, -8.0106338e-04, -2.2795475e-04, ...,\n",
       "         -4.0653371e-04,  2.6256189e-04, -2.0669219e-04],\n",
       "        [-7.9980027e-04, -1.8439391e-03, -4.9613335e-04, ...,\n",
       "         -1.1603427e-03,  6.9538585e-04, -2.3018967e-04],\n",
       "        [-2.9706254e-04, -2.3288250e-03, -9.2918548e-04, ...,\n",
       "         -1.8256443e-03,  1.1343141e-03, -4.4002503e-04],\n",
       "        ...,\n",
       "        [ 1.0412221e-03, -6.1656919e-04,  1.0588117e-04, ...,\n",
       "         -1.6003032e-03,  1.0733911e-03, -1.5600625e-03],\n",
       "        [ 2.5784798e-04,  5.1969320e-05,  4.6138320e-04, ...,\n",
       "         -9.0370793e-04,  8.1587775e-04, -1.8022910e-03],\n",
       "        [-6.7717658e-04,  7.6404185e-04,  8.2114653e-04, ...,\n",
       "         -2.7423899e-04,  6.0070091e-04, -1.9723487e-03]],\n",
       "\n",
       "       [[-2.6503956e-04, -8.0106338e-04, -2.2795475e-04, ...,\n",
       "         -4.0653371e-04,  2.6256189e-04, -2.0669219e-04],\n",
       "        [-3.9707232e-04, -1.9786411e-03, -8.0475473e-04, ...,\n",
       "         -5.1626918e-04,  2.9978348e-04, -2.4874797e-04],\n",
       "        [-5.4028368e-04, -2.8997615e-03, -1.4088057e-03, ...,\n",
       "         -4.4995375e-04,  5.7297939e-04, -6.2727160e-04],\n",
       "        ...,\n",
       "        [-4.3570907e-03,  2.9328554e-03,  1.0548395e-03, ...,\n",
       "          1.9978848e-03, -5.3481950e-04, -2.2185210e-03],\n",
       "        [-4.8331562e-03,  3.6058335e-03,  1.1545487e-03, ...,\n",
       "          1.9208780e-03, -6.0928764e-04, -2.1366470e-03],\n",
       "        [-5.2194456e-03,  4.1958485e-03,  1.1965154e-03, ...,\n",
       "          1.8063000e-03, -6.6560501e-04, -2.0567970e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.6503956e-04, -8.0106338e-04, -2.2795475e-04, ...,\n",
       "         -4.0653371e-04,  2.6256189e-04, -2.0669219e-04],\n",
       "        [-6.4436701e-04, -1.5031315e-03, -2.0139503e-04, ...,\n",
       "         -5.0201663e-04, -2.1123997e-04, -1.1041475e-04],\n",
       "        [-1.0816242e-03, -2.3256068e-03, -4.1685192e-04, ...,\n",
       "         -4.0992748e-04, -5.0654187e-04,  2.3124318e-04],\n",
       "        ...,\n",
       "        [-4.0028016e-03,  2.7142523e-03,  1.4463754e-03, ...,\n",
       "          2.2300235e-03, -7.5910689e-04, -1.0900188e-03],\n",
       "        [-4.6476652e-03,  3.3635614e-03,  1.6582516e-03, ...,\n",
       "          2.3223602e-03, -7.1829639e-04, -1.3578610e-03],\n",
       "        [-5.1987544e-03,  3.9247535e-03,  1.7844022e-03, ...,\n",
       "          2.3354387e-03, -6.9811620e-04, -1.5501253e-03]],\n",
       "\n",
       "       [[-2.6503956e-04, -8.0106338e-04, -2.2795475e-04, ...,\n",
       "         -4.0653371e-04,  2.6256189e-04, -2.0669219e-04],\n",
       "        [-4.0277495e-04, -9.2622224e-04, -6.8331201e-04, ...,\n",
       "         -1.0942152e-03,  3.8620637e-04, -1.6621962e-04],\n",
       "        [-5.9615762e-04, -6.7800673e-04, -8.2747487e-04, ...,\n",
       "         -1.6720127e-03,  6.8903028e-04, -8.4703992e-04],\n",
       "        ...,\n",
       "        [-4.2136521e-03,  4.2547988e-05, -8.0180512e-04, ...,\n",
       "         -2.5125956e-03,  1.3462218e-03, -8.9042973e-05],\n",
       "        [-4.4532586e-03,  9.8969605e-05, -8.0960034e-04, ...,\n",
       "         -1.9745796e-03,  1.0851803e-03, -7.6804170e-04],\n",
       "        [-4.5814556e-03,  5.3858332e-04, -4.8568839e-04, ...,\n",
       "         -1.2680794e-03,  8.6087885e-04, -1.3767160e-03]],\n",
       "\n",
       "       [[-2.6503956e-04, -8.0106338e-04, -2.2795475e-04, ...,\n",
       "         -4.0653371e-04,  2.6256189e-04, -2.0669219e-04],\n",
       "        [-1.8053227e-04, -9.3051634e-04, -2.6632860e-04, ...,\n",
       "         -8.5811131e-04, -1.6005259e-05,  1.5536496e-04],\n",
       "        [-3.0307687e-04, -6.4971001e-04, -2.0598978e-04, ...,\n",
       "         -1.3512177e-03,  1.2657384e-04, -3.7880114e-04],\n",
       "        ...,\n",
       "        [-2.6039581e-03,  2.8723455e-03,  4.5199695e-04, ...,\n",
       "          8.3539623e-04, -1.6886337e-03, -1.1973084e-04],\n",
       "        [-3.3485054e-03,  3.3631527e-03,  6.7334116e-04, ...,\n",
       "          1.1377127e-03, -1.6393070e-03, -5.5134320e-04],\n",
       "        [-4.0491857e-03,  3.7973253e-03,  8.8840659e-04, ...,\n",
       "          1.3328486e-03, -1.5779722e-03, -8.7904953e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "christian-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  15073256  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  26056000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  32008000  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  24014001  \n",
      "=================================================================\n",
      "Total params: 97,151,257\n",
      "Trainable params: 97,151,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "infrared-meeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "3901/3901 [==============================] - 1107s 283ms/step - loss: 3.1695 - val_loss: 2.6096\n",
      "Epoch 2/7\n",
      "3901/3901 [==============================] - 1109s 284ms/step - loss: 2.3792 - val_loss: 2.3633\n",
      "Epoch 3/7\n",
      "3901/3901 [==============================] - 1107s 284ms/step - loss: 1.8900 - val_loss: 2.2375\n",
      "Epoch 4/7\n",
      "3901/3901 [==============================] - 1108s 284ms/step - loss: 1.5283 - val_loss: 2.1963\n",
      "Epoch 5/7\n",
      "3901/3901 [==============================] - 1110s 285ms/step - loss: 1.2954 - val_loss: 2.2097\n",
      "Epoch 6/7\n",
      "3901/3901 [==============================] - 1108s 284ms/step - loss: 1.1596 - val_loss: 2.2540\n",
      "Epoch 7/7\n",
      "3901/3901 [==============================] - 1109s 284ms/step - loss: 1.0868 - val_loss: 2.2982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1f79e53390>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 학습\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "#Loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=7, validation_data=(enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "labeled-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 평가하기\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):    \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:  \n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        \n",
    "        if predict_word.numpy()[0]    == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\" \n",
    "    \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "nuclear-muscle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love it when you call me big poppa <end> '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#생성 함수 실행\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
