{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b93UQUvfZghP"
   },
   "source": [
    "# Using pre-trained word embeddings\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2020/05/05<br>\n",
    "**Last modified:** 2020/05/05<br>\n",
    "**Description:** Text classification on the Newsgroup20 dataset using pre-trained GloVe word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4As3JvSoZghU"
   },
   "source": [
    "##설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AV2-49gUZghV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swRRL4gfZghX"
   },
   "source": [
    "## 소개\n",
    "\n",
    "이 예에서는 사전 훈련된 단어 임베딩을 사용하는 텍스트 분류 모델을 훈련하는 방법을 보여줍니다.\n",
    "\n",
    "20개의 다른 주제 범주에 속하는 20,000개의 게시판 메시지 세트인 Newsgroup20 데이터 세트로 작업하겠습니다.\n",
    "\n",
    "사전 훈련된 단어 임베딩의 경우 GloVe 임베딩을 사용 합니다.\n",
    "\n",
    "[GloVe embeddings](http://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe\n",
    "-  벡터 차이가 두 단어의 병치에 의해 지정된 의미를 최대한 포착하도록 설계\n",
    "- 전역 단어-단어 동시 발생 행렬의 0이 아닌 항목에 대해 학습되며, 이 행렬은 주어진 말뭉치에서 단어가 서로 얼마나 자주 동시 발생하는지 표를 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TXllEiAZghY"
   },
   "source": [
    "## Newsgroup20 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBofTpHvZghY",
    "outputId": "5ebcbd02-76e3-48a3-f2dd-df9820e5c9f0"
   },
   "outputs": [],
   "source": [
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvPmy-DlZghZ"
   },
   "source": [
    "## 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0i7zUc6RZgha",
    "outputId": "5b7d9ab8-60a9-4160-eab9-2c12c9f0b477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 20\n",
      "Directory names: ['talk.religion.misc', 'talk.politics.mideast', 'sci.crypt', 'misc.forsale', 'comp.os.ms-windows.misc', 'comp.sys.mac.hardware', 'sci.space', 'comp.sys.ibm.pc.hardware', 'sci.electronics', 'rec.sport.hockey', 'talk.politics.misc', 'talk.politics.guns', 'rec.sport.baseball', 'alt.atheism', 'rec.autos', 'comp.graphics', 'sci.med', 'comp.windows.x', 'rec.motorcycles', 'soc.religion.christian']\n",
      "Number of files in comp.graphics: 1000\n",
      "Some example filenames: ['38596', '38770', '38333', '38432', '38744']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "print(\"Number of directories:\", len(dirnames))\n",
    "print(\"Directory names:\", dirnames)\n",
    "\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")  #숨김파일 .keras/datasets/20_newgroup/comp.graphics 세로 오름차순정렬된 filenames 확인됨\n",
    "print(\"Number of files in comp.graphics:\", len(fnames))\n",
    "print(\"Some example filenames:\", fnames[:5])  # 랜덤 값인것처럼 배치되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgUjgJNfZghc"
   },
   "source": [
    "다음은 한 파일에 포함된 내용의 예입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYiK9KIoZghd",
    "outputId": "f686ef31-5e6d-481d-8225-54bf5fd5824e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsgroups: comp.graphics\n",
      "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!dog.ee.lbl.gov!network.ucsd.edu!usc!rpi!nason110.its.rpi.edu!mabusj\n",
      "From: mabusj@nason110.its.rpi.edu (Jasen M. Mabus)\n",
      "Subject: Looking for Brain in CAD\n",
      "Message-ID: <c285m+p@rpi.edu>\n",
      "Nntp-Posting-Host: nason110.its.rpi.edu\n",
      "Reply-To: mabusj@rpi.edu\n",
      "Organization: Rensselaer Polytechnic Institute, Troy, NY.\n",
      "Date: Thu, 29 Apr 1993 23:27:20 GMT\n",
      "Lines: 7\n",
      "\n",
      "Jasen Mabus\n",
      "RPI student\n",
      "\n",
      "\tI am looking for a hman brain in any CAD (.dxf,.cad,.iges,.cgm,etc.) or picture (.gif,.jpg,.ras,etc.) format for an animation demonstration. If any has or knows of a location please reply by e-mail to mabusj@rpi.edu.\n",
      "\n",
      "Thank you in advance,\n",
      "Jasen Mabus  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(open(data_dir / \"comp.graphics\" / \"38987\").read()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organization: Rensselaer Polytechnic Institute, Troy, NY. ->조직: 렌셀러 폴리테크닉 인스티튜트, 트로이, 뉴욕."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhdZh343Zghe"
   },
   "source": [
    "보시다시피 명시적으로(첫 번째 줄은 말 그대로 범주 이름임) 또는 암시적으로(예: 파일을 통해) 파일의 범주를 누출하는 헤더 줄이 있습니다 Organization. 헤더를 제거합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdkX-cn6Zghe",
    "outputId": "7b18b597-61c2-4c5a-ab3c-264e5948d425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing alt.atheism, 1000 files found\n",
      "Processing comp.graphics, 1000 files found\n",
      "Processing comp.os.ms-windows.misc, 1000 files found\n",
      "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
      "Processing comp.sys.mac.hardware, 1000 files found\n",
      "Processing comp.windows.x, 1000 files found\n",
      "Processing misc.forsale, 1000 files found\n",
      "Processing rec.autos, 1000 files found\n",
      "Processing rec.motorcycles, 1000 files found\n",
      "Processing rec.sport.baseball, 1000 files found\n",
      "Processing rec.sport.hockey, 1000 files found\n",
      "Processing sci.crypt, 1000 files found\n",
      "Processing sci.electronics, 1000 files found\n",
      "Processing sci.med, 1000 files found\n",
      "Processing sci.space, 1000 files found\n",
      "Processing soc.religion.christian, 997 files found\n",
      "Processing talk.politics.guns, 1000 files found\n",
      "Processing talk.politics.mideast, 1000 files found\n",
      "Processing talk.politics.misc, 1000 files found\n",
      "Processing talk.religion.misc, 1000 files found\n",
      "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of samples: 19997\n"
     ]
    }
   ],
   "source": [
    "# 샘플 목록에 for 문을 활용하여 라벨, 클래스명, 클래스 인덱스 형식으로 정리, 클래스명, 샘플수 확인\n",
    "samples = []\n",
    "labels = []\n",
    "class_names = []\n",
    "class_index = 0\n",
    "\n",
    "for dirname in sorted(os.listdir(data_dir)):\n",
    "    class_names.append(dirname)\n",
    "    dirpath = data_dir / dirname\n",
    "    fnames = os.listdir(dirpath)\n",
    "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
    "    for fname in fnames:\n",
    "        fpath = dirpath / fname\n",
    "        f = open(fpath, encoding=\"latin-1\")\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        lines = lines[10:]\n",
    "        content = \"\\n\".join(lines)\n",
    "        samples.append(content)\n",
    "        labels.append(class_index)\n",
    "    class_index += 1\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Number of samples:\", len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC8394r2Zghf"
   },
   "source": [
    "실제로 예상되는 파일 수가 없는 범주가 하나 있지만 그 차이가 충분히 작아 문제가 균형 잡힌 분류 문제로 남아 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWPe_46tZghg"
   },
   "source": [
    "## 데이터를 섞고 훈련 및 검증 세트로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kjBQoAM2Zghg"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "seed = 1337  # seed 어떤 값을 넣던지 별 차이 없이 numpy로 하여금 서로 다른 유사난수 생성\n",
    "rng = np.random.RandomState(seed) #np.random.RandomState() \n",
    "rng.shuffle(samples)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# Extract a training & validation split\n",
    "validation_split = 0.2  #유효성 검사를 위해 데이터의 20%를 사용\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples = samples[-num_validation_samples:]\n",
    "train_labels = labels[:-num_validation_samples]\n",
    "val_labels = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>np.random.RandomState()Permalink</b>\n",
    "- 특정 seed를 가지는 np.random.RandomState()를 만들어주고, 여기서부터 이 object에 접근하여 난수를 생성\n",
    "\n",
    "<b>np.random.random()</b>\n",
    "- numpy에 존재하는 random generator에 직접 접근하여, 난수를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9lUUwVWZghi"
   },
   "source": [
    "## 어휘 색인 만들기\n",
    "\n",
    "<b>TextVectorization</b>\n",
    "- 데이터 세트에서 찾은 어휘를 색인화 하는 데 사용, 추후 동일 레이어 인스턴트사용 샘플 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "4bC_jZ_HZghi",
    "outputId": "2e7e1fff-f875-44ac-cc0e-f03ebca4d9b4"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# 레이어의 상위 2만단어 고려, 시퀀스를 자르거나 채워서 실제로 200개 토큰 길이 맞추기\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "\n",
    "# 대용량의 데이터에서 배치 단위로 데이터를 가져오기 위한 데이터셋 제너레이터를 생성하는 코드\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfAP20HYZghj"
   },
   "source": [
    "이를 통해 사용된 계산 어휘를 검색 할 수 있습니다.  `vectorizer.get_vocabulary()`. 상위 5개 단어를 인쇄 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7h134evDZghj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'to', 'of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상위 5개 어휘 검색\n",
    "\n",
    "vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgeymPrMZghj"
   },
   "source": [
    "테스트 문장을 벡터화합시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ma11SwFyZghk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 3315, 1823,   15,    2, 6018])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 문장 벡터화하기\n",
    "\n",
    "output = vectorizer([[\"the cat sat on the mat\"]])\n",
    "output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_ACK9Z7Zghk"
   },
   "source": [
    "보시다시피 \"\"는 \"2\"로 표시됩니다. \"\"가 어휘의 첫 번째 단어라는 점을 감안할 때 0이 아닌 이유는 무엇입니까? 인덱스 0은 패딩용으로 예약되어 있고 인덱스 1은 \"어휘 외\" 토큰용으로 예약되어 있기 때문입니다.\n",
    "\n",
    "다음은 단어를 인덱스에 매핑하는 사전입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "s7N7ysxhZghl"
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcajp5-9Zghl"
   },
   "source": [
    "보시다시피 테스트 문장에 대해 위와 동일한 인코딩을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KwUwoMQhZghm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3315, 1823, 15, 2, 6018]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "[word_index[w] for w in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krpj_F89Zghm"
   },
   "source": [
    "## 사전 훈련된 단어 임베딩 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21jBCc-mZghm"
   },
   "source": [
    "사전 훈련된 GloVe 임베딩(822M zip 파일)을 다운로드해 보겠습니다.\n",
    "\n",
    "다음 명령을 실행해야 합니다.\n",
    "\n",
    "```\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdNJRAYuZghn"
   },
   "source": [
    "아카이브에는 50차원, 100차원, 200차원, 300차원 등 다양한 크기의 텍스트로 인코딩된 벡터가 포함되어 있습니다. 우리는 100D를 사용할 것입니다.\n",
    "\n",
    "NumPy 벡터 표현에 단어(문자열)를 매핑하는 딕셔너리를 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fZgyB1I8Zgho"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = os.path.join(\n",
    "    os.path.expanduser(\"~\"), \"aiffel/nlp/glove.6B.100d.txt\"\n",
    ")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvhxiHZYZgho"
   },
   "source": [
    "이제 Keras Embedding레이어 에서 사용할 수 있는 해당 임베딩 매트릭스를 준비하겠습니다 . 그것은 인덱스 항목이 단순 NumPy와 매트릭스의 i인덱스의 말씀에 대한 사전 훈련 벡터이다 i우리의 vectorizer의 어휘."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "35Mag7p1Zgho"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 17998 words (2002 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXyHEXTXZghp"
   },
   "source": [
    "다음으로 사전 훈련된 단어 임베딩 행렬을 Embedding레이어에 로드합니다 .\n",
    "\n",
    "trainable=False임베딩을 고정된 상태로 유지하도록 설정했습니다 (교육 중에 업데이트하지 않으려고 함)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yvx7xAlYZghp"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7sMIumzZghp"
   },
   "source": [
    "## 모델 구축\n",
    "\n",
    "전역 최대 풀링과 끝에 분류기가 있는 간단한 1D convnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LiWdMQ8AZghq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         2000200   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 2,247,516\n",
      "Trainable params: 247,316\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeUFs20oZghq"
   },
   "source": [
    "## 모델구축\n",
    "\n",
    "먼저 문자열 목록 데이터를 정수 인덱스의 NumPy 배열로 변환합니다. 배열은 오른쪽으로 채워집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SRmMCpEEZghq"
   },
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7jYkSYsZghq"
   },
   "source": [
    "우리는 softmax 분류를 수행하기 때문에 범주형 교차 엔트로피를 손실로 사용합니다. 또한 sparse_categorical_crossentropy레이블이 정수이기 때문에 사용 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1Uqg8BHZghr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 9s 64ms/step - loss: 2.6689 - acc: 0.1325 - val_loss: 2.0422 - val_acc: 0.2896\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 1.9441 - acc: 0.3266 - val_loss: 1.6753 - val_acc: 0.4369\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 1.5270 - acc: 0.4731 - val_loss: 1.3630 - val_acc: 0.5269\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 8s 63ms/step - loss: 1.2655 - acc: 0.5697 - val_loss: 1.1894 - val_acc: 0.5976\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 8s 63ms/step - loss: 1.0956 - acc: 0.6292 - val_loss: 1.1345 - val_acc: 0.6207\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 0.9668 - acc: 0.6720 - val_loss: 1.1254 - val_acc: 0.6227\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 0.8533 - acc: 0.7072 - val_loss: 1.0697 - val_acc: 0.6552\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 0.7369 - acc: 0.7468 - val_loss: 1.0959 - val_acc: 0.6432\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 0.6537 - acc: 0.7760 - val_loss: 1.1357 - val_acc: 0.6679\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 0.5679 - acc: 0.8023 - val_loss: 1.0640 - val_acc: 0.6759\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 0.4878 - acc: 0.8305 - val_loss: 1.1335 - val_acc: 0.6874\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 8s 63ms/step - loss: 0.4343 - acc: 0.8497 - val_loss: 1.1813 - val_acc: 0.6814\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 0.3672 - acc: 0.8715 - val_loss: 1.1011 - val_acc: 0.6979\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 8s 63ms/step - loss: 0.3335 - acc: 0.8855 - val_loss: 1.1878 - val_acc: 0.7057\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 8s 63ms/step - loss: 0.2892 - acc: 0.9015 - val_loss: 1.2198 - val_acc: 0.7074\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 8s 64ms/step - loss: 0.2552 - acc: 0.9124 - val_loss: 1.3587 - val_acc: 0.6912\n",
      "Epoch 17/20\n",
      " 65/125 [==============>...............] - ETA: 3s - loss: 0.2299 - acc: 0.9222"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVnnXx0NZghr"
   },
   "source": [
    "## 종단 간 모델 내보내기\n",
    "\n",
    "이제 Model일련의 인덱스가 아니라 임의 길이의 문자열을 입력 으로 사용하는 객체 를 내보내고 싶을 수 있습니다 . 입력 전처리 파이프라인에 대해 걱정할 필요가 없기 때문에 모델을 훨씬 더 이식성 있게 만들 수 있습니다.\n",
    "\n",
    "우리 vectorizer는 실제로 Keras 레이어이므로 간단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwAPZWQOZghr"
   },
   "outputs": [],
   "source": [
    "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = vectorizer(string_input)\n",
    "preds = model(x)\n",
    "end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "probabilities = end_to_end_model.predict(\n",
    "    [[\"this message is about computer graphics and 3D modeling\"]]\n",
    ")\n",
    "\n",
    "class_names[np.argmax(probabilities[0])]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pretrained_word_embeddings",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
